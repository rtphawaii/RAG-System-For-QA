# RAG-System-For-QA
Experimenting with RAG

# Executive Summary
A RAG (Retrieval-Augmented Generation) system operates as a question-answering framework that delivers responses grounded in contextual proprietary information, minimizing hallucinations and ensuring up-to-date accuracy. A basic RAG system consists of several key components: a chunking strategy, embeddings, a vector database, a retriever, a prompt template, and a large language model for text generation. By identifying weaknesses in this naive setup and addressing them with advanced techniques, a more robust system can be developed to produce more accurate and reliable text responses.
# Introduction
This proof-of-concept report addresses the challenge of effectively utilizing proprietary company documentation for question answering. Companies often aim to reduce reliance on tribal knowledge to bridge the knowledge gap that hinders new employees from making an immediate impact. New hires are frequently overwhelmed by the need to process vast amounts of job-critical information. In a sea of documentation, identifying relevant information for specific questions can be difficult and time-consuming. Even when the right document is found, the information is often so dense that extracting the necessary details for a given task becomes a significant challenge.

Large language models are good at what they do. They can take a prompt and generate a response cohesively and succinctly. They are also capable of generating detailed step by step instructions and long form summaries. One thing that they are not capable of is reliably providing current and accurate information. They also struggle with hallucination and often generate responses that read logically and sound correct but are wildly offbase in actuality. A rag system constricts an LLM by providing up to date and relevant context that the LLM’s response is conditioned on. A rag system can also be modified to generate responses that are appropriate to different audiences in the amount of information provided and the tone of the response. In this report the rag system aims to provide question answering services to both research and marketing teams.
# Key Findings
RAG systems improve the response to questions that can be answered by portions of a knowledge corpus
Chunking can be improved by breaking a document into its elements and using a mixture of semantic chunking and recursive text splitting
Embedding models that are pre-trained on a diverse set of question-answer pairs generate a more reliable and accurate way of indexing documents 
Retrieval strategies such as cosine similarity, query expansion, query augmentation, and reranking improve response accuracy significantly
Prompt engineering also improves performance quantitatively and qualitatively
# Methodology
The approach for this project involved developing a baseline model and iteratively enhancing it by addressing its weaknesses. The baseline model utilized a naive RAG system, which stored and retrieved documents as embedding vectors and generated responses based on the retrieved documents using an LLM. Subsequent improvements focused on resolving the baseline system's shortcomings, resulting in measurable enhancements to response metrics, both individually and on average.

This report highlights several key areas of weakness in a RAG system that can be addressed. A RAG system depends on critical components such as chunking, embedding, retrieval, prompt engineering, and inference. Each of these components presents unique challenges for developers, which are explored and resolved in this work.
# Technical Approach
Chunking is used to segment documents and context into smaller portions of text and metadata. During this process there are a number of considerations. Mainly the rag system needs to be able to answer questions that may pull contextual information from several parts of a large document or from entirely different documents. On the other hand the LLM may find difficulties in sifting through the provided contextual information if the chunks that they are delivered in are too lengthy and contain a lot of irrelevant information (or in other words, noise). Another issue that cropped up while developing the rag system was that certain chunks contained a lot of key words but were useless as far as actual content goes. These types of chunks are deceptive and misleading sources of noise. The first two issues regarding noise and ample context present a trade off. Providing more characters/tokens in an effort to increase contextual information introduces the possibility of noise also being introduced into the chunks. Smaller chunk sizes are less likely to have irrelevant information but they are more likely to lack the required context needed to accurately answer the question. Finding a balanced chunk size helps to achieve better responses and testing a range of values for chunk size and overlap helps to identify the right ratio. Aside from chunk size and overlap there are chunking strategies that influence how documents are split into chunks. Fixed size chunking breaks down the text into chunks of a specified number of characters, regardless of their content or structure. Iterative chunking divides the text into smaller chunks in a hierarchical and iterative manner using a set of separators. Document based chunking splits the text into sections like titles, tables/charts, and body text. Recursive chunking splits text based on special characters that identify parts of the text like paragraphs and whitespace. If the initial attempt at splitting the text doesn’t produce chunks of the desired size, the method recursively calls itself on the resulting chunks with a different separator until the desired chunk size is achieved. Each chunking strategy has advantages, the two mentioned are basic and are the least computationally expensive. The desired chunking strategy is largely case and situation based and the choice depends on the structure and content of the text. I chose to use a combination of semantic chunking, document based chunking, and recursive chunking. The reason I chose to do this was because the main sources of information were research papers, articles, and wikipedia pages. Each of these sources included sections like code segments, tables and graphs. The most detrimental section to RAG performance was the works cited section. The works cited section contained lots of keywords that lacked any explanation or true content. The unstructured api for chunking splits the documents into elements including titles, headers, tables/charts, body paragraphs, and works cited. By segmenting the documents into elements I was able to retain the body text and get rid of all the noise. The unstructured api also uses semantic chunking which uses an embedding model and a threshold to determine whether a chunk of text is explaining a singular topic or if it is branching off into a new topic with different semantic meaning. If the threshold is exceeded a split is made to reserve one topic per chunk. Lastly metadata was appended to each document object to help with querying the vector database further. 

Once documents have been chunked in a desirable way, the embedding model is used to create a vector representation of each chunk that serves as a key index for the chunk to be semantically searched and retrieved from within the vector database. The embedding model leverages the transformer architecture to tokenize text inputs and generate dense representations that encode the semantic meaning of the text. These representations are expressed as multi-dimensional coordinates, positioning the text within a vector space relative to other texts. The most basic way of taking a query and finding documentation to produce an answer is by using keyword searching. Keyword searching uses the number of matches for each unigram in the query (or possibly n-grams in more sophisticated keyword search algorithms) to rank documents that might be relevant to answering the question that was posed. The issue with keyword searching is that most of the time the document sharing the most keywords with the query does not contain the answer to the query itself. For example, "Who wrote Hamlet" does not share any words with its answer, "William Shakespeare." The solution to this is to create representations of the relevant documents that exist in a multi-dimensional space and capture the semantic meaning of the document. An embedding model is pre-trained to take a chunk of text and convert it into a multi-dimensional vector representation. Similar chunks of text are vectors that occupy positions that are close together, for example, "the capital of Hawaii is Honolulu" and "the capital of California is Sacramento" will likely be close in proximity. The vector that is generated is used to create an index position for the document that can be searched by meaning in place of matching keywords. As a whole process - the documents that are needed as context are chunked using a desired strategy, the embedding model converts those chunks to vectors, and finally the chunks are stored in a vector database with their index as the embedding vector that was generated to represent it. When it comes time to search, the same embedding model is used to convert the query to an embedding of the same dimensions and then the retriever takes the embedded query and finds the top k most semantically similar documents using cosine similarity.

As far as Embeddings go, we were given the choice of several different embedding models to use. Each of the embedding models were trained on labeled question answering pairs that were sourced from various internet resources. Among the most notable sources were wikipedia, yahoo answers, reddit, and quora. The first embedding model, multi-qa-mpnet-base-dot-v1, is intended for semantic search. It encodes queries and text paragraphs in a dense vector space. Text inputs into the model are returned as 768 dimensional vectors. The embedding model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. Given a sentence pair, the model should predict the sentence that is paired with the original sentence. 215 million question answer pairs are used to train the model. The question answer training tuples come from websites like wikianswers, stackexchange, and yahoo answers. Using the inference API on huggingface I conducted a test of sentence similarities for a given query. I found that conditional on the question, the similarity scores for the question itself were higher than the actual answer to the question where. Given that this was the case I reworded the questions into hypothetical answers before using a retriever to semantically search for documents that are related to the question. This is contrastive to what I would've expected since the embedding model is trained on question and answer pairs. It would've made sense for answers to be most similar to the paired questions but that was not the case. It is worth considering that the all-mpnet-base-v2 has more training examples and uses the same base model as multi-qa-mpnet-base-dot-v1. However, based on my testing, multi-qa-mpnet-base-dot-v1 performs better for the applications in this project. The choice of vector database is also highly important. We were not tasked with deliberating between databases but in the future it would be possible to explore graph databases as a major improvement. In the future I would also like to try fine-tuning the embedding model or adding an adapter to the retriever. 

Retrieval of documents is done naively by semantic search. Given a query, the embedding model converts the query into a vector and several comparison methods can be used to find the most relevant chunk in the vector database. The main measures of comparison are euclidean distance, cosine similarity, and dot product similarity. For simplicity I used cosine similarity to retrieve relevant documents for each query. Beyond semantic search there are several methods that I used to retrieve better context. By expanding the query into an n number of slightly different queries, the retriever can be called n times to find n*k documents (in the project I expanded into 3 queries and retrieved a total of 15 documents). The queries can also be changed and rewritten as answers so that they are more semantically similar to the chunks that are likely to be written in answer form. By combining both methods the system is able to cast a wide net and retrieve a large number of documents that may be relevant to the answer. Given a larger scope of context it can be assumed that noise has also increased in the collection. Using a reranking system can be used to eliminate noise and irrelevant documents, trimming down to only what is necessary to provide a correct answer. I experimented with two different reranking systems, one that uses the BERT cross-encoder, and also Cohere’s reranking api. Each reranking system takes 15 documents and returns the top 5 scoring as context. In the future scope of this project I would like to experiment with sub-querying and making additional calls for retrieval while generating a text response - or in other words, iterative retrieval. 

Lastly once context and documentation are retrieved they are injected into the LLM coupled with a prompt using LangChain. The context is bundled and formatted into a segmented body of text and prompt engineering can be used to get the LLM to generate better responses. A good prompt should be tailored by adding several components: Audience, Role, Task, Output Characteristics, Output Format, and “Do Nots”. The audience in the case of this project is a technically savvy team that requires details and also a customer facing team that simplifies complex topics into digestible and concise marketing deliverables. The role of the LLM should be as an engineering/marketing researcher. The task is to provide either a detailed or high level summary that answers the question using the provided context. Some “Do Nots” that should be listed include hallucination and answering unfaithfully given the provided context.
# Testing and Evaluation 
Testing and Evaluation was done in two phases. The first phase was testing by eye and reading the responses generated by the LLM to determine which hyper parameters would work best for the task at hand. The second phase of testing was using defined metrics including BERTscore, BLEU, ROUGE, and RAGAS. BERT, ROUGE, and BLEU all have precision recall and f-1 scores. The success of the project is gauged by whether or not there is a measurable difference after employing chunking, retrieval, and prompt engineering strategies in multiple configurations. 

BERTScore evaluates text similarity based on contextual embeddings rather than token matching. It’s typically used for evaluation on tasks like text generation and summarization. The candidate and reference texts are converted to embeddings using the BERT transformer architecture and similarity is calculated using cosine similarity. BERTScore consists of  precision, recall, and f-1 scores. Precision captures how well the generated tokens are matched/covered by the reference tokens on average. Recall captures how well the reference tokens are covered/matched by the generated tokens on average. F-1 is the harmonic mean of precision and recall. BERTScore is good because it captures contextual meaning and semantic similarity. However it may overestimate similarity in cases of shallow paraphrasing.

BLEU measures the similarity between a machine-generated text and one or more reference texts based on n-gram overlap. The approach works by counting matching n-grams in the candidate translation to n-grams in the reference text, where 1-gram or unigram would be each token and a bigram comparison would be each word pair. The comparison is made regardless of word order. Even though BLEU is intended for machine translation, many still use it as a benchmark (coupled with others) for question answering. The biggest con of the metric is that it ignores semantic meaning, but this is addressed by other metrics.

ROUGE measures overlap between machine-generated summaries and reference summaries. ROUGE-N: Measures n-gram overlap (e.g., ROUGE-1 for unigrams, ROUGE-2 for bigrams). ROUGE-L: Uses longest common subsequence (LCS) to capture sentence-level fluency. The metric is well suited for summarization tasks and it is flexible on different n-gram levels. Each ROUGE score calculates precision, recall, and an f-1 score. The only downside is that ROUGE also overlooks semantic meaning. 

RAGAS is a composite metric for evaluating candidate and reference texts for question answering tasks. RAGAS is the metric of choice and it is an LLM as a judge type of measure. RAGAS combines multiple evaluation techniques including embedding similarity, lexical overlap, and other semantic scoring methods. RAGAS can also be used to evaluate the context provided and faithfulness of the answer generated. For this project GPT 3.5 turbo is used to judge the correctness between the gold answer and the candidate text that is generated. Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. These aspects are combined using a weighted scheme to formulate the answer correctness score. RAGAS is a unique measure that provides insights beyond lexical comparison and semantic similarity (that are provided by BERTScore and ROUGE and BLEU). I chose RAGAS answer correctness because it compares the generated answer and the ground truth (gold answers). Answer relevance compares the generated answer to the question so I decided not to use it. In the future I would add more RAGAS metrics to understand the context retrieval quality and answer faithfulness to the context.

